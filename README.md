realtime-data-pipeline/
â”‚â”€â”€ README.md
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ producer/
â”‚   â””â”€â”€ kafka_producer.py
â”‚â”€â”€ consumer/
â”‚   â””â”€â”€ spark_streaming.py
â”‚â”€â”€ data/
â”‚   â””â”€â”€ sample_data.json

# Real-Time Data Pipeline with Kafka & Spark  

ğŸš€ This project demonstrates a real-time data pipeline using **Kafka** as a message broker and **Apache Spark Structured Streaming** for processing.  

## âš™ï¸ Architecture
1. A **Kafka Producer** simulates streaming data (JSON events).  
2. Kafka acts as a **message broker**.  
3. **Spark Streaming** consumes the data, transforms it, and prints it (can also write to BigQuery, PostgreSQL, etc.).  

## ğŸ› ï¸ Tech Stack
- Python
- Apache Kafka
- Apache Spark
- Docker Compose  

## â–¶ï¸ How to Run
1. Start Kafka & Zookeeper:
   ```bash
   docker-compose up -d
